{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本教程将介绍使用PyTorch的Hybrid Frontend将seq2seq模型转换为Torch Script的过程。 我们将转换的模型是Chatbot教程中的聊天机器人模型。 您可以将本教程视为Chatbot教程的“第2部分”并部署您自己的预训练模型，也可以从本文档开始并使用我们托管的预训练模型。 在后一种情况下，您可以参考原始的Chatbot教程，以获取有关数据预处理，模型理论和定义以及模型训练的详细信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the Hybrid Fronted?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在基于深度学习的项目的研究和开发阶段，与PyTorch等eager、必要的接口进行交互是有利的，这使用户能够编写熟悉的、符合习惯的Python代码，允许使用Python数据结构，控制流操作，打印语句和debug等实用功能。虽然eager界面是研究和实验应用程序的有益工具，但是当在生产环境中部署模型时，使用基于图形的模型表示非常有益。延迟图（the deferred graph）表示允许优化，例如无序执行，以及针对高度优化的硬件架构的能力。此外，基于图形的表示使框架无关模型的导出成为可能。 PyTorch提供了将eager模式代码逐步转换为Torch Script的机制，Torch Script是一个静态可分析且可优化的Python子集，Torch使用它来独立于Python运行时表示深度学习程序。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用于将eager模式的PyTorch程序转换为Torch Script的API可在`torch.jit`模块中找到。该模块有两种核心模式，用于将eager模式模型转换为Torch Script图形表示：`tracing`和`scripting`。 `torch.jit.trace`函数接受一个模块或函数以及一组示例输入，然后，它在跟踪遇到的计算步骤时通过函数或模块运行示例输入，并输出执行跟踪操作的基于图形的函数。`tracing`非常适用于不涉及数据相关控制流的简单模块和函数，例如标准卷积神经网络。但是，如果跟踪具有依赖于数据的if语句和循环的函数，则仅记录由示例输入执行的执行路径调用的操作，换句话说，不捕获控制流本身。为了转换包含依赖于数据的控制流的模块和函数，提供了一种scripting机制。Scripting显式将模块或函数代码转换为Torch Script，包括所有可能的控制流路径。要使用脚本模式，请确保从`torch.jit.ScriptModule`基类（而不是torch.nn.Module）继承，并将`torch.jit.script`装饰器添加到Python函数或将torch.jit.script_method装饰器中添加到模块的方法中。使用Scripting的一个警告是它只支持Python的受限子集,有关支持的功能的所有详细信息，请参阅Torch Script语言参考。为了提供最大的灵活性，可以组合Torch Script的模式来表示整个程序，并且可以逐步应用这些技术。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，我们导入一些必要的库包，并且设置一些常量。如果你计划使用自己的模型，请确保`MAX_LENGTH`常量设置正确。提醒一下，它定义为在训练过程中允许的句子长度的最大值以及产生的模型的最大输出长度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "MAX_LENGTH = 10  # Maximum sentence length\n",
    "\n",
    "# Default word tokens\n",
    "PAD_token = 0  # Used for padding short sentences\n",
    "SOS_token = 1  # Start-of-sentence token\n",
    "EOS_token = 2  # End-of-sentence token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Overiew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们这里使用的是seq2seq模型，在这种情况下我们的输入时变长的序列，输出也是变长的序列，它们之间不需要是一对一的映射。seq2seq模型式由两个RNN组成，一个叫做编码器，一个叫做解码器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "编码器RNN在输入上进行迭代：一个时间步上迭代一个标记（例如，词），在每个时间步输出一个输出向量和一个隐藏状态向量。 然后将隐藏状态向量传递到下一个时间步，同时输出向量也被记录下来。 编码器将其在序列中的每个点处看到的上下文变换为高维空间中的一组点，解码器将使用这些数字来为给定任务生成有意义的输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解码器RNN以逐个标记的方式生成应答语句。 它使用编码器的上下文向量和内部隐藏状态来生成序列中的下一个词。 它继续生成单词，直到它输出一个表示句子结尾的EOS标记。 我们在解码器中使用注意机制来帮助它在生成输出时“关注”输入的某些部分。 对于我们的模型，我们实现了Luong等人的“Global Attention”模块，并将其用作解码模型中的子模块。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尽管我们的模型在概念上处理的是序列化的标记，但实际上它们处理的是数字，就像其他机器学习模型一样。在此情况下，词汇表中的每一个词都会在模型训练开始前，被映射为一个整数索引。我们使用类Voc来实现从单词到索引的转换。我们将会在运行模型前加载这个类。\n",
    "\n",
    "同样的，为了能够很好的评估，我们必须提供一个工具来处理我们的字符输入。函数`normalizeString`将所有的字符转换为小写形式，并且移除了所有的非字母标识。`indexesFromSentence`函数接收句子中的词作为输入，并返回每个词对应的索引序列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Voc:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.trimmed = False\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3  # Count SOS, EOS, PAD\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    # Remove words below a certain count threshold\n",
    "    def trim(self, min_count):\n",
    "        if self.trimmed:\n",
    "            return\n",
    "        self.trimmed = True\n",
    "        keep_words = []\n",
    "        for k, v in self.word2count.items():\n",
    "            if v >= min_count:\n",
    "                keep_words.append(k)\n",
    "\n",
    "        print('keep_words {} / {} = {:.4f}'.format(\n",
    "            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n",
    "        ))\n",
    "        # Reinitialize dictionaries\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3 # Count default tokens\n",
    "        for word in keep_words:\n",
    "            self.addWord(word)\n",
    "\n",
    "\n",
    "# Lowercase and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "\n",
    "# Takes string sentence, returns sentence of word indexes\n",
    "def indexesFromSentence(voc, sentence):\n",
    "    return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用`torch.nn.GRU`模块实现编码器的RNN，我们提供一个batch大小的句子（词嵌入的向量），并在内部迭代句子，每个时间步一个标记，计算隐藏状态。我们将此模块初始化为双向的，这意味着我们有两个独立的GRU：一个按时间顺序迭代序列，另一个以相反的顺序迭代。我们最终返回这两个GRU输出的总和。由于我们的模型是使用批处理训练的，因此我们的EncoderRNN模型的正向函数需要对输入的batch进行填充。对于变长的batch的句子，我们允许句子中最多MAX_LENGTH个标记，并且批量中小于MAX_LENGTH个标记的所有句子都在末尾用我们专用的PAD_token标记填充。要使用带有PyTorch RNN模块的填充批处理，我们必须使用`torch.nn.utils.rnn.pack_padded_sequence`和`torch.nn.utils.rnn.pad_packed_sequence`数据转换来包装forward pass调用。请注意，forward函数还采用input_lengths列表，其中包含批处理中每个句子的长度。填充时，此输入由`torch.nn.utils.rnn.pack_padded_sequence`函数使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hybrid Frontend Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "\n",
    "        # Initialize GRU; the input_size and hidden_size params are both set to 'hidden_size'\n",
    "        #   because our input size is a word embedding with number of features == hidden_size\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n",
    "                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n",
    "\n",
    "    def forward(self, input_seq, input_lengths, hidden=None):\n",
    "        # Convert word indexes to embeddings\n",
    "        embedded = self.embedding(input_seq)\n",
    "        # Pack padded batch of sequences for RNN module\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "        # Forward pass through GRU\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        # Unpack padding\n",
    "        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        # Sum bidirectional GRU outputs\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
    "        # Return output and final hidden state\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Decoder's Attention Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们将定义我们的注意力模块（Attn）。 请注意，此模块将用作解码器模型中的子模块。Luong等人考虑各种“得分函数”，其取当前解码器RNN输出和整个编码器输出，并返回注意力“能量”。 该注意能量向量与编码器输出的大小相同，并且这两者最终相乘，产生加权张量，其最大值表示在解码的特定时间步骤中查询句子的最重要部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Luong attention layer\n",
    "class Attn(torch.nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = method\n",
    "        if self.method not in ['dot', 'general', 'concat']:\n",
    "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
    "        self.hidden_size = hidden_size\n",
    "        if self.method == 'general':\n",
    "            self.attn = torch.nn.Linear(self.hidden_size, hidden_size)\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = torch.nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.v = torch.nn.Parameter(torch.FloatTensor(hidden_size))\n",
    "\n",
    "    def dot_score(self, hidden, encoder_output):\n",
    "        return torch.sum(hidden * encoder_output, dim=2)\n",
    "\n",
    "    def general_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(encoder_output)\n",
    "        return torch.sum(hidden * energy, dim=2)\n",
    "\n",
    "    def concat_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n",
    "        return torch.sum(self.v * energy, dim=2)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # Calculate the attention weights (energies) based on the given method\n",
    "        if self.method == 'general':\n",
    "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'concat':\n",
    "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'dot':\n",
    "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
    "\n",
    "        # Transpose max_length and batch_size dimensions\n",
    "        attn_energies = attn_energies.t()\n",
    "\n",
    "        # Return the softmax normalized probability scores (with added dimension)\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与EncoderRNN类似，我们使用`torch.nn.GRU`模块作为解码器的RNN。 但是，这一次，我们使用单向GRU。 重要的是要注意，与编码器不同，我们将一次向解码器RNN提供一个词。 我们首先获取当前单词的词嵌入并应用dropout。 接下来，我们将词嵌入和上个隐藏状态输入到GRU并获得当前GRU输出和隐藏状态。 然后我们使用我们的Attn模块作为一个层来获得注意权重，我们将其乘以编码器的输出以获得我们的attended encoder output。 我们使用这个attended encoder output作为我们的上下文向量，它表示加权和，指示编码器输出的哪些部分需要被注意。之后，我们使用线性层和softmax标准化来选择输出序列中的下一个字。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hybrid Frontend Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与EncoderRNN类似，此模块不包含任何与数据相关的控制流。 因此，在初始化并加载其参数后，我们可以再次使用tracing将此模型转换为Torch Script。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super(LuongAttnDecoderRNN, self).__init__()\n",
    "\n",
    "        # Keep for reference\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Define layers\n",
    "        self.embedding = embedding\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
    "        # Note: we run this one step (word) at a time\n",
    "        # Get embedding of current input word\n",
    "        embedded = self.embedding(input_step)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        # Forward through unidirectional GRU\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "        # Calculate attention weights from the current GRU output\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        # Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
    "        # Concatenate weighted context vector and GRU output using Luong eq. 5\n",
    "        rnn_output = rnn_output.squeeze(0)\n",
    "        context = context.squeeze(1)\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "        # Predict next word using Luong eq. 6\n",
    "        output = self.out(concat_output)\n",
    "        output = F.softmax(output, dim=1)\n",
    "        # Return output and final hidden state\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Evalution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "~~待补充~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy Search Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hybrid Frontend Notes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedySearchDecoder(torch.jit.ScriptModule):\n",
    "    def __init__(self, encoder, decoder, decoder_n_layers):\n",
    "        super(GreedySearchDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self._device = device\n",
    "        self._SOS_token = SOS_token\n",
    "        self._decoder_n_layers = decoder_n_layers\n",
    "\n",
    "    __constants__ = ['_device', '_SOS_token', '_decoder_n_layers']\n",
    "\n",
    "    @torch.jit.script_method\n",
    "    def forward(self, input_seq : torch.Tensor, input_length : torch.Tensor, max_length : int):\n",
    "        # Forward input through encoder model\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
    "        # Prepare encoder's final hidden layer to be first hidden input to the decoder\n",
    "        decoder_hidden = encoder_hidden[:self._decoder_n_layers]\n",
    "        # Initialize decoder input with SOS_token\n",
    "        decoder_input = torch.ones(1, 1, device=self._device, dtype=torch.long) * self._SOS_token\n",
    "        # Initialize tensors to append decoded words to\n",
    "        all_tokens = torch.zeros([0], device=self._device, dtype=torch.long)\n",
    "        all_scores = torch.zeros([0], device=self._device)\n",
    "        # Iteratively decode one word token at a time\n",
    "        for _ in range(max_length):\n",
    "            # Forward pass through decoder\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            # Obtain most likely word token and its softmax score\n",
    "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
    "            # Record token and score\n",
    "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
    "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
    "            # Prepare current token to be next decoder input (add a dimension)\n",
    "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
    "        # Return collections of word tokens and scores\n",
    "        return all_tokens, all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating an Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH):\n",
    "    ### Format input sentence as a batch\n",
    "    # words -> indexes\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence)]\n",
    "    # Create lengths tensor\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    # Transpose dimensions of batch to match models' expectations\n",
    "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
    "    # Use appropriate device\n",
    "    input_batch = input_batch.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    # Decode sentence with searcher\n",
    "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
    "    # indexes -> words\n",
    "    decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
    "    return decoded_words\n",
    "\n",
    "\n",
    "# Evaluate inputs from user input (stdin)\n",
    "def evaluateInput(encoder, decoder, searcher, voc):\n",
    "    input_sentence = ''\n",
    "    while(1):\n",
    "        try:\n",
    "            # Get input sentence\n",
    "            input_sentence = input('> ')\n",
    "            # Check if it is quit case\n",
    "            if input_sentence == 'q' or input_sentence == 'quit': break\n",
    "            # Normalize sentence\n",
    "            input_sentence = normalizeString(input_sentence)\n",
    "            # Evaluate sentence\n",
    "            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
    "            # Format and print response sentence\n",
    "            output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
    "            print('Bot:', ' '.join(output_words))\n",
    "\n",
    "        except KeyError:\n",
    "            print(\"Error: Encountered unknown word.\")\n",
    "\n",
    "# Normalize input sentence and call evaluate()\n",
    "def evaluateExample(sentence, encoder, decoder, searcher, voc):\n",
    "    print(\"> \" + sentence)\n",
    "    # Normalize sentence\n",
    "    input_sentence = normalizeString(sentence)\n",
    "    # Evaluate sentence\n",
    "    output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
    "    output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
    "    print('Bot:', ' '.join(output_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pretrained Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n"
     ]
    }
   ],
   "source": [
    "save_dir = os.path.join(\"data\", \"save\")\n",
    "corpus_name = \"cornell movie-dialogs corpus\"\n",
    "\n",
    "# Configure models\n",
    "model_name = 'cb_model'\n",
    "attn_model = 'dot'\n",
    "#attn_model = 'general'\n",
    "#attn_model = 'concat'\n",
    "hidden_size = 500\n",
    "encoder_n_layers = 2\n",
    "decoder_n_layers = 2\n",
    "dropout = 0.1\n",
    "batch_size = 64\n",
    "\n",
    "# If you're loading your own model\n",
    "# Set checkpoint to load from\n",
    "checkpoint_iter = 4000\n",
    "# loadFilename = os.path.join(save_dir, model_name, corpus_name,\n",
    "#                             '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size),\n",
    "#                             '{}_checkpoint.tar'.format(checkpoint_iter))\n",
    "\n",
    "# If you're loading the hosted model\n",
    "\n",
    "loadFilename = './data/4000_checkpoint.tar'\n",
    "\n",
    "\n",
    "# Load model\n",
    "# Force CPU device options (to match tensors in this tutorial)\n",
    "checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
    "encoder_sd = checkpoint['en']\n",
    "decoder_sd = checkpoint['de']\n",
    "encoder_optimizer_sd = checkpoint['en_opt']\n",
    "decoder_optimizer_sd = checkpoint['de_opt']\n",
    "embedding_sd = checkpoint['embedding']\n",
    "voc = Voc(corpus_name)\n",
    "voc.__dict__ = checkpoint['voc_dict']\n",
    "\n",
    "\n",
    "print('Building encoder and decoder ...')\n",
    "# Initialize word embeddings\n",
    "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
    "embedding.load_state_dict(embedding_sd)\n",
    "# Initialize encoder & decoder models\n",
    "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\n",
    "# Load trained model params\n",
    "encoder.load_state_dict(encoder_sd)\n",
    "decoder.load_state_dict(decoder_sd)\n",
    "# Use appropriate device\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "# Set dropout layers to eval mode\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "print('Models built and ready to go!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Model to Torch Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GreedySearchDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Convert encoder model\n",
    "# Create artificial inputs\n",
    "test_seq = torch.LongTensor(MAX_LENGTH, 1).random_(0, voc.num_words)\n",
    "test_seq_length = torch.LongTensor([test_seq.size()[0]])\n",
    "# Trace the model\n",
    "traced_encoder = torch.jit.trace(encoder, (test_seq, test_seq_length))\n",
    "\n",
    "### Convert decoder model\n",
    "# Create and generate artificial inputs\n",
    "test_encoder_outputs, test_encoder_hidden = traced_encoder(test_seq, test_seq_length)\n",
    "test_decoder_hidden = test_encoder_hidden[:decoder.n_layers]\n",
    "test_decoder_input = torch.LongTensor(1, 1).random_(0, voc.num_words)\n",
    "# Trace the model\n",
    "traced_decoder = torch.jit.trace(decoder, (test_decoder_input, test_decoder_hidden, test_encoder_outputs))\n",
    "\n",
    "### Initialize searcher module\n",
    "scripted_searcher = GreedySearchDecoder(traced_encoder, traced_decoder, decoder.n_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scripted_searcher graph:\n",
      " graph(%input_seq : Tensor\n",
      "      %input_length : Tensor\n",
      "      %max_length : int\n",
      "      %3 : Tensor\n",
      "      %4 : Tensor\n",
      "      %5 : Tensor\n",
      "      %6 : Tensor\n",
      "      %7 : Tensor\n",
      "      %8 : Tensor\n",
      "      %9 : Tensor\n",
      "      %10 : Tensor\n",
      "      %11 : Tensor\n",
      "      %12 : Tensor\n",
      "      %13 : Tensor\n",
      "      %14 : Tensor\n",
      "      %15 : Tensor\n",
      "      %16 : Tensor\n",
      "      %17 : Tensor\n",
      "      %18 : Tensor\n",
      "      %19 : Tensor\n",
      "      %118 : Tensor\n",
      "      %119 : Tensor\n",
      "      %120 : Tensor\n",
      "      %121 : Tensor\n",
      "      %122 : Tensor\n",
      "      %123 : Tensor\n",
      "      %124 : Tensor\n",
      "      %125 : Tensor\n",
      "      %126 : Tensor\n",
      "      %127 : Tensor\n",
      "      %128 : Tensor\n",
      "      %129 : Tensor\n",
      "      %130 : Tensor) {\n",
      "  %58 : int = prim::Constant[value=9223372036854775807](), scope: EncoderRNN\n",
      "  %53 : float = prim::Constant[value=0](), scope: EncoderRNN\n",
      "  %43 : float = prim::Constant[value=0.1](), scope: EncoderRNN/GRU[gru]\n",
      "  %42 : int = prim::Constant[value=2](), scope: EncoderRNN/GRU[gru]\n",
      "  %41 : bool = prim::Constant[value=1](), scope: EncoderRNN/GRU[gru]\n",
      "  %36 : int = prim::Constant[value=6](), scope: EncoderRNN/GRU[gru]\n",
      "  %34 : int = prim::Constant[value=500](), scope: EncoderRNN/GRU[gru]\n",
      "  %25 : int = prim::Constant[value=4](), scope: EncoderRNN\n",
      "  %24 : Device = prim::Constant[value=\"cpu\"](), scope: EncoderRNN\n",
      "  %21 : bool = prim::Constant[value=0](), scope: EncoderRNN/Embedding[embedding]\n",
      "  %20 : int = prim::Constant[value=-1](), scope: EncoderRNN/Embedding[embedding]\n",
      "  %90 : int = prim::Constant[value=0]()\n",
      "  %94 : int = prim::Constant[value=1]()\n",
      "  %input.7 : Float(10, 1, 500) = aten::embedding(%3, %input_seq, %20, %21, %21), scope: EncoderRNN/Embedding[embedding]\n",
      "  %lengths : Long(1) = aten::to(%input_length, %24, %25, %21, %21), scope: EncoderRNN\n",
      "  %input.1 : Float(10, 500), %batch_sizes : Long(10) = aten::_pack_padded_sequence(%input.7, %lengths, %21), scope: EncoderRNN\n",
      "  %35 : int[] = prim::ListConstruct(%25, %94, %34), scope: EncoderRNN/GRU[gru]\n",
      "  %hx : Float(4, 1, 500) = aten::zeros(%35, %36, %90, %24), scope: EncoderRNN/GRU[gru]\n",
      "  %40 : Tensor[] = prim::ListConstruct(%4, %5, %6, %7, %8, %9, %10, %11, %12, %13, %14, %15, %16, %17, %18, %19), scope: EncoderRNN/GRU[gru]\n",
      "  %46 : Float(10, 1000), %encoder_hidden : Float(4, 1, 500) = aten::gru(%input.1, %batch_sizes, %hx, %40, %41, %42, %43, %21, %41), scope: EncoderRNN/GRU[gru]\n",
      "  %49 : int = aten::size(%batch_sizes, %90), scope: EncoderRNN\n",
      "  %max_seq_length : Long() = prim::NumToTensor(%49), scope: EncoderRNN\n",
      "  %51 : int = prim::TensorToNum(%max_seq_length), scope: EncoderRNN\n",
      "  %outputs : Float(10, 1, 1000), %55 : Long(1) = aten::_pad_packed_sequence(%46, %batch_sizes, %21, %53, %51), scope: EncoderRNN\n",
      "  %60 : Float(10, 1, 1000) = aten::slice(%outputs, %90, %90, %58, %94), scope: EncoderRNN\n",
      "  %65 : Float(10, 1, 1000) = aten::slice(%60, %94, %90, %58, %94), scope: EncoderRNN\n",
      "  %70 : Float(10, 1!, 500) = aten::slice(%65, %42, %90, %34, %94), scope: EncoderRNN\n",
      "  %75 : Float(10, 1, 1000) = aten::slice(%outputs, %90, %90, %58, %94), scope: EncoderRNN\n",
      "  %80 : Float(10, 1, 1000) = aten::slice(%75, %94, %90, %58, %94), scope: EncoderRNN\n",
      "  %85 : Float(10, 1!, 500) = aten::slice(%80, %42, %34, %58, %94), scope: EncoderRNN\n",
      "  %encoder_outputs : Float(10, 1, 500) = aten::add(%70, %85, %94), scope: EncoderRNN\n",
      "  %decoder_hidden.1 : Tensor = aten::slice(%encoder_hidden, %90, %90, %42, %94)\n",
      "  %98 : int[] = prim::ListConstruct(%94, %94)\n",
      "  %100 : Tensor = aten::ones(%98, %25, %90, %24)\n",
      "  %decoder_input.1 : Tensor = aten::mul(%100, %94)\n",
      "  %103 : int[] = prim::ListConstruct(%90)\n",
      "  %all_tokens.1 : Tensor = aten::zeros(%103, %25, %90, %24)\n",
      "  %108 : int[] = prim::ListConstruct(%90)\n",
      "  %all_scores.1 : Tensor = aten::zeros(%108, %36, %90, %24)\n",
      "  %all_scores : Tensor, %all_tokens : Tensor, %decoder_hidden : Tensor, %decoder_input : Tensor = prim::Loop(%max_length, %41, %all_scores.1, %all_tokens.1, %decoder_hidden.1, %decoder_input.1)\n",
      "    block0(%114 : int, %188 : Tensor, %184 : Tensor, %116 : Tensor, %115 : Tensor) {\n",
      "      %input.2 : Float(1, 1, 500) = aten::embedding(%118, %115, %20, %21, %21), scope: LuongAttnDecoderRNN/Embedding[embedding]\n",
      "      %input.3 : Float(1, 1, 500) = aten::dropout(%input.2, %43, %21), scope: LuongAttnDecoderRNN/Dropout[embedding_dropout]\n",
      "      %138 : Tensor[] = prim::ListConstruct(%119, %120, %121, %122, %123, %124, %125, %126), scope: LuongAttnDecoderRNN/GRU[gru]\n",
      "      %hidden : Float(1, 1, 500), %decoder_hidden.2 : Float(2, 1, 500) = aten::gru(%input.3, %116, %138, %41, %42, %43, %21, %21, %21), scope: LuongAttnDecoderRNN/GRU[gru]\n",
      "      %147 : Float(10, 1, 500) = aten::mul(%hidden, %encoder_outputs), scope: LuongAttnDecoderRNN/Attn[attn]\n",
      "      %149 : int[] = prim::ListConstruct(%42), scope: LuongAttnDecoderRNN/Attn[attn]\n",
      "      %attn_energies : Float(10, 1) = aten::sum(%147, %149, %21), scope: LuongAttnDecoderRNN/Attn[attn]\n",
      "      %input.4 : Float(1!, 10) = aten::t(%attn_energies), scope: LuongAttnDecoderRNN/Attn[attn]\n",
      "      %154 : Float(1, 10) = aten::softmax(%input.4, %94), scope: LuongAttnDecoderRNN/Attn[attn]\n",
      "      %attn_weights : Float(1, 1, 10) = aten::unsqueeze(%154, %94), scope: LuongAttnDecoderRNN/Attn[attn]\n",
      "      %159 : Float(1!, 10, 500) = aten::transpose(%encoder_outputs, %90, %94), scope: LuongAttnDecoderRNN\n",
      "      %context.1 : Float(1, 1, 500) = aten::bmm(%attn_weights, %159), scope: LuongAttnDecoderRNN\n",
      "      %rnn_output : Float(1, 500) = aten::squeeze(%hidden, %90), scope: LuongAttnDecoderRNN\n",
      "      %context : Float(1, 500) = aten::squeeze(%context.1, %94), scope: LuongAttnDecoderRNN\n",
      "      %165 : Tensor[] = prim::ListConstruct(%rnn_output, %context), scope: LuongAttnDecoderRNN\n",
      "      %input.5 : Float(1, 1000) = aten::cat(%165, %94), scope: LuongAttnDecoderRNN\n",
      "      %168 : Float(1000!, 500!) = aten::t(%127), scope: LuongAttnDecoderRNN/Linear[concat]\n",
      "      %171 : Float(1, 500) = aten::addmm(%128, %input.5, %168, %94, %94), scope: LuongAttnDecoderRNN/Linear[concat]\n",
      "      %input.6 : Float(1, 500) = aten::tanh(%171), scope: LuongAttnDecoderRNN\n",
      "      %173 : Float(500!, 7826!) = aten::t(%129), scope: LuongAttnDecoderRNN/Linear[out]\n",
      "      %input : Float(1, 7826) = aten::addmm(%130, %input.6, %173, %94, %94), scope: LuongAttnDecoderRNN/Linear[out]\n",
      "      %decoder_output : Float(1, 7826) = aten::softmax(%input, %94), scope: LuongAttnDecoderRNN\n",
      "      %decoder_scores : Tensor, %decoder_input.2 : Tensor = aten::max(%decoder_output, %94, %21)\n",
      "      %186 : Tensor[] = prim::ListConstruct(%184, %decoder_input.2)\n",
      "      %all_tokens.2 : Tensor = aten::cat(%186, %90)\n",
      "      %190 : Tensor[] = prim::ListConstruct(%188, %decoder_scores)\n",
      "      %all_scores.2 : Tensor = aten::cat(%190, %90)\n",
      "      %decoder_input.3 : Tensor = aten::unsqueeze(%decoder_input.2, %90)\n",
      "      -> (%41, %all_scores.2, %all_tokens.2, %decoder_hidden.2, %decoder_input.3)\n",
      "    }\n",
      "  return (%all_tokens, %all_scores);\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('scripted_searcher graph:\\n', scripted_searcher.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> hello\n",
      "Bot: hello .\n",
      "> what's up?\n",
      "Bot: i m going to get my car .\n",
      "> who are you?\n",
      "Bot: i m the owner .\n",
      "> where am I?\n",
      "Bot: in the house .\n",
      "> where are you from?\n",
      "Bot: south america .\n"
     ]
    }
   ],
   "source": [
    "# Evaluate examples\n",
    "sentences = [\"hello\", \"what's up?\", \"who are you?\", \"where am I?\", \"where are you from?\"]\n",
    "for s in sentences:\n",
    "    evaluateExample(s, traced_encoder, traced_decoder, scripted_searcher, voc)\n",
    "\n",
    "# Evaluate your input\n",
    "#evaluateInput(traced_encoder, traced_decoder, scripted_searcher, voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scripted_searcher.save(\"scripted_chatbot.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
